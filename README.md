## ğŸ§¾ Fake Product Review Detection using Machine Learning

Detecting **Computer-Generated (CG)** fake product reviews versus **Original (OR)** human-written ones using machine learning models trained on 40,000 labeled reviews.

---

### ğŸ¯ Objective

The goal of this project is to build a robust machine learning pipeline that can:

* Accurately distinguish fake (CG) reviews from real (OR) reviews
* Provide real-time predictions through a Streamlit web app
* Offer interpretable insights into what makes a review appear "fake"

---

### âš™ï¸ Methodology

#### ğŸ”„ Preprocessing

* Lowercasing, 
* punctuation & digit removal
* Stopword removal
* **TF-IDF Vectorization** 

#### ğŸ¤– Models Trained

* Logistic Regression
* Linear SVM
* Multinomial Naive Bayes
* Random Forest Classifier

#### ğŸ§ª Evaluation

* 5-fold cross-validation
* Hyperparameter tuning via `GridSearchCV`
* Final testing on a held-out test set of 8,087 reviews

---

### ğŸ“Š Model Performance (on Test Set)

| Model               | Accuracy | Precision | Recall | F1-Score | Best Hyperparameters                     |
| ------------------- | -------- | --------- | ------ | -------- | ---------------------------------------- |
| Logistic Regression | 0.88     | 0.88      | 0.88   | 0.88     | `solver=liblinear`, `C=10`, `penalty=l2` |
| Linear SVM          | 0.88     | 0.88      | 0.88   | 0.88     | `loss=hinge`, `C=1`, `max_iter=1000`     |
| Naive Bayes         | 0.85     | 0.85      | 0.85   | 0.85     | default                                  |
| Random Forest       | 0.86     | 0.86      | 0.86   | 0.86     | (grid search results not shown)          |

> âœ… **Top Performers**: Logistic Regression and Linear SVM (F1 = 0.88)

---

### ğŸ§  Text Feature Insights

#### ğŸ”´ Top Fake Review Indicators:

`developed`, `wide`, `couple`, `materials`, `onei`, `downside`, `admit`, `problem`, `reason`, `iti`

> These words are often vague, technical, or oddly placed â€” suggesting computer generation.

#### ğŸŸ¢ Top Real Review Indicators:

`though`, `even`, `without`, `instead`, `connects`, `ask`, `coming`, `bc`, `fitting`, `due`

> These are more conversational and natural, typical of genuine user experiences.

---

### ğŸ–¥ï¸ Streamlit Web App

An interactive web application is included to:

* ğŸ§¾ **Predict** whether a new review is Fake or Real
* ğŸ“Š **Visualize model performance** (confusion matrix, classification report, comparison bar and pie chart)
* ğŸ§  **Explore linguistic patterns** with top indicative words and sample reviews

#### ğŸ› ï¸ App Structure

```
ğŸ“‚ project/
â”œâ”€â”€ app/
â”‚   â””â”€â”€ streamlit_app.py                # Streamlit app
â”œâ”€â”€ data/
â”‚   â”œâ”€â”€ fake_reviews.csv                # Original data
â”‚   â”œâ”€â”€ cleaned_reviews.csv             # Processed data
â”‚   â””â”€â”€ stopwords_en.json               # stop words
â”œâ”€â”€ model/
â”‚   â”œâ”€â”€ X_train.pkl                     # Cached datasets
â”‚   â”œâ”€â”€ X_test.pkl                      # Cached datasets
â”‚   â”œâ”€â”€ y_train.pkl                     # Cached datasets
â”‚   â”œâ”€â”€ y_test.pkl                      # Cached datasets
â”‚   â”œâ”€â”€ tfidf.pkl                       # TF-IDF vectorizer
â”‚   â””â”€â”€ model.pkl                       # Trained model
â”œâ”€â”€ notebooks/
â”‚   â”œâ”€â”€ fake_reviews.ipynb
â”‚   â””â”€â”€ logistic_regression.ipynb
â”œâ”€â”€ utils/
â”‚   â”œâ”€â”€ __init__.py
â”‚   â””â”€â”€ clean_text.py                   # Text preprocessing functions
â”œâ”€â”€ requirements.txt
â”œâ”€â”€ README.md
```

#### ğŸ”— How to Run

```bash
pip install -r requirements.txt
streamlit run app/main.py
```

---

### ğŸ§ª Training Pipeline

1. Preprocess & vectorize text
2. Train 4 ML models
3. Evaluate using precision, recall, F1-score
4. Fine-tune with GridSearchCV
5. Save best model and vectorizer with `joblib`
6. Deploy via Streamlit app

---

### ğŸ“¦ Requirements

* Python 3.8+
- ipykernel
- numpy
- pandas
- matplotlib
- seaborn
- scikit-learn
- nltk
- joblib
- streamlit

Install all dependencies:

```bash
pip install -r requirements.txt
```

---

### ğŸ“Œ Conclusion

* Both **Logistic Regression** and **Linear SVM** achieved the best F1-score of **0.88**
* The models effectively learn subtle linguistic patterns between CG and OR reviews
* The web app enables real-time predictions and explainability for end users

---

### ğŸ“ Repository Overview

```
ğŸ“‚ project-root/
â”œâ”€â”€ app/
â”‚   â””â”€â”€ streamlit_app.py
â”œâ”€â”€ data/
â”‚   â”œâ”€â”€ fake_reviews.csv
â”‚   â”œâ”€â”€ cleaned_reviews.csv
â”‚   â””â”€â”€ stopwords_en.json
â”œâ”€â”€ model/
â”‚   â”œâ”€â”€ X_train.pkl
â”‚   â”œâ”€â”€ X_test.pkl
â”‚   â”œâ”€â”€ y_train.pkl
â”‚   â”œâ”€â”€ y_test.pkl
â”‚   â”œâ”€â”€ tfidf.pkl
â”‚   â””â”€â”€ model.pkl
â”œâ”€â”€ notebooks/
â”‚   â”œâ”€â”€ fake_reviews.ipynb
â”‚   â””â”€â”€ logistic_regression.ipynb
â”œâ”€â”€ utils/
â”‚   â”œâ”€â”€ __init__.py
â”‚   â””â”€â”€ clean_text.py
â”œâ”€â”€ requirements.txt
â”œâ”€â”€ README.md
```

---

